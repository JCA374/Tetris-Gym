# test_with_better_strategy.py
"""Test with a smarter action selection strategy"""

import sys
sys.path.insert(0, '/home/claude')

from config import make_env
import numpy as np

def test_smart_placement():
    """Use a smarter strategy to spread pieces horizontally"""
    print("="*80)
    print("ðŸ§  TESTING SMART PLACEMENT STRATEGY")
    print("="*80)
    
    env = make_env(render_mode=None, use_complete_vision=True)
    
    # Strategy: Systematically place pieces in different columns
    # Move LEFT/RIGHT to position, then hard drop
    
    placement_strategies = [
        # (description, action_sequence)
        ("Far left", [0, 0, 0, 0, 5]),  # Move left 4x, drop
        ("Left", [0, 0, 5]),  # Move left 2x, drop
        ("Center-left", [0, 5]),  # Move left 1x, drop
        ("Center", [5]),  # Just drop
        ("Center-right", [1, 5]),  # Move right 1x, drop
        ("Right", [1, 1, 5]),  # Move right 2x, drop
        ("Far right", [1, 1, 1, 1, 5]),  # Move right 4x, drop
    ]
    
    total_lines = 0
    
    for attempt in range(5):  # Try 5 full attempts
        obs, _ = env.reset()
        
        print(f"\nðŸŽ® Attempt {attempt + 1}:")
        
        pieces_placed = 0
        done = False
        
        # Cycle through placement strategies
        strategy_idx = 0
        
        while not done and pieces_placed < 30:
            strategy_name, actions = placement_strategies[strategy_idx % len(placement_strategies)]
            
            # Execute action sequence
            for action in actions:
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                
                if done:
                    break
            
            pieces_placed += 1
            strategy_idx += 1
            
            # Check for lines
            lines = info.get('number_of_lines', 0)
            if lines > 0:
                total_lines += lines
                print(f"   ðŸŽ‰ {lines} LINE(S) CLEARED after {pieces_placed} pieces!")
                print(f"   Used strategy: {strategy_name}")
            
            if done:
                print(f"   Game over after {pieces_placed} pieces")
                break
        
        # Check board state
        board = obs[:, :, 0]
        filled_cells = np.count_nonzero(board)
        
        # Analyze row fullness
        max_row_fullness = 0
        for row in board:
            fullness = np.count_nonzero(row)
            max_row_fullness = max(max_row_fullness, fullness)
        
        print(f"   Pieces placed: {pieces_placed}")
        print(f"   Total cells filled: {filled_cells}")
        print(f"   Max row fullness: {max_row_fullness}/10")
    
    env.close()
    
    print(f"\n" + "="*80)
    print(f"ðŸ“Š RESULTS:")
    print(f"   Total lines cleared: {total_lines}")
    
    if total_lines > 0:
        print(f"   âœ… SUCCESS! Lines CAN be cleared with better strategy!")
        print(f"\n   ðŸ’¡ SOLUTION: Your agent needs to learn to:")
        print(f"      1. Move pieces LEFT/RIGHT before dropping")
        print(f"      2. Spread pieces across columns")
        print(f"      3. Not just spam hard drop")
    else:
        print(f"   âŒ Still 0 lines even with smart placement")
        print(f"   This suggests tetris-gymnasium may not be suitable for RL")


def test_even_distribution():
    """Test filling the board more evenly"""
    print("\n" + "="*80)
    print("ðŸŽ¯ TESTING EVEN DISTRIBUTION")
    print("="*80)
    
    env = make_env(render_mode=None, use_complete_vision=True)
    obs, _ = env.reset()
    
    # Strategy: Alternate between left and right
    actions_sequence = []
    for i in range(20):
        if i % 4 == 0:
            actions_sequence.extend([0, 0, 5])  # Left, left, drop
        elif i % 4 == 1:
            actions_sequence.extend([5])  # Center drop
        elif i % 4 == 2:
            actions_sequence.extend([1, 1, 5])  # Right, right, drop
        else:
            actions_sequence.extend([1, 5])  # Right, drop
    
    print("\n   Executing balanced placement sequence...")
    
    step = 0
    total_lines = 0
    
    for action in actions_sequence:
        obs, reward, terminated, truncated, info = env.step(action)
        step += 1
        
        lines = info.get('number_of_lines', 0)
        if lines > 0:
            total_lines += lines
            print(f"   ðŸŽ‰ {lines} LINE(S) at step {step}!")
        
        if terminated or truncated:
            print(f"   Game ended at step {step}")
            break
    
    board = obs[:, :, 0]
    
    # Analyze final board
    print(f"\n   Final board analysis:")
    for row_idx in range(min(10, board.shape[0])):
        row = board[-(row_idx+1), :]  # From bottom up
        fullness = np.count_nonzero(row)
        print(f"      Row {row_idx}: {'â–ˆ' * int(fullness)} ({fullness}/10)")
    
    print(f"\n   Total lines cleared: {total_lines}")
    
    env.close()
    
    return total_lines > 0


if __name__ == "__main__":
    result1 = test_smart_placement()
    result2 = test_even_distribution()
    
    print("\n" + "="*80)
    print("ðŸŽ¯ FINAL VERDICT")
    print("="*80)
    
    print("""
The issue is clear: Random actions don't spread pieces horizontally!

CRITICAL INSIGHT:
- Action 5 (hard drop) places piece in current column
- Without moving LEFT/RIGHT first, pieces stack in same spot
- Game ends after 3-5 pieces before any row fills

YOUR AGENT MUST LEARN TO:
1. Move pieces horizontally (actions 0=LEFT, 1=RIGHT)
2. THEN drop (action 5)
3. Balance piece placement across all columns

This is actually good news - it means:
âœ… Environment works correctly
âœ… Your agent just needs better action selection
âœ… Reward shaping should encourage horizontal movement

RECOMMENDED FIXES:
1. Add reward for horizontal distribution
2. Penalize placing pieces in already-full columns
3. Increase exploration time (slower epsilon decay)
4. Use action masking to prevent bad drops
    """)
